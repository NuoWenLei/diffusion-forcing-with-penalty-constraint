defaults:
  - base_pytorch

training:
  lr: 5e-4
  precision: 16-mixed
  batch_size: 128 # will use 16G GPU memory;
  max_steps: 202005 # 50k steps should be enough for large, and 100k for medium

  checkpointing:
    every_n_train_steps: 500 # save a checkpoint every n train steps

validation:
  precision: 32
  inference_mode: False
  batch_size: 128
  val_every_n_step: 500 # change to 1 when visualizing a trained checkpoint
  val_every_n_epoch: null # change to null when visualizing a trained checkpoint
  limit_batch: 1
