{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jY4VE8YzJMO8"
      },
      "source": [
        "# This is adapted from the work of [Halafi, 2024](https://arxiv.org/abs/2408.15094).\n",
        "\n",
        "Original GitHub repo: https://github.com/shervinkhalafi/Constrained_Diffusion_Dual_Training\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "Nwii8ZJZKCjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYJTm4eJJMO-"
      },
      "outputs": [],
      "source": [
        "#Import Required Libraries\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "from dataclasses import dataclass\n",
        "import wandb\n",
        "from datasets import load_dataset\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torch\n",
        "from diffusers import UNet2DModel\n",
        "from diffusers import AutoencoderKL\n",
        "from diffusers import DDPMScheduler, DDIMScheduler\n",
        "import torch.nn.functional as F\n",
        "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
        "from diffusers import DDIMPipeline\n",
        "from diffusers.utils import make_image_grid\n",
        "from accelerate import Accelerator\n",
        "from tqdm.auto import tqdm\n",
        "from pathlib import Path\n",
        "from accelerate import notebook_launcher\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "import torchvision\n",
        "# from github import Github\n",
        "from transformers import CLIPProcessor, CLIPModel, CLIPTokenizer\n",
        "from transformers import ViTFeatureExtractor, ViTForImageClassification, ViTImageProcessor\n",
        "import shutil\n",
        "from itertools import cycle\n",
        "from transformers import CLIPProcessor, CLIPModel, CLIPTokenizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Download Pretrained MNIST Diffusion Model"
      ],
      "metadata": {
        "id": "UXMlZRpNHrqo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown '1-c06A48YcgGqGy4Zq8hszmJiUQ9WWTPi'"
      ],
      "metadata": {
        "id": "8w1EJOtaHKFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "mkdir /content/output\n",
        "mkdir /content/output/models\n",
        "mkdir /content/output/samples"
      ],
      "metadata": {
        "id": "KuPrKhzfH4Rf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uIaJrt9bJMO_"
      },
      "outputs": [],
      "source": [
        "#Configuration of Training hyperparameters\n",
        "@dataclass\n",
        "\n",
        "class TrainingConfig:\n",
        "\n",
        "    adaptation = False #If adaptation = True, the problem will be treated as a adaptation/fine-tuning problem where we want to adapt a pre-trained model to new data without overfitting.\n",
        "\n",
        "    image_size = 32 # the generated image resolution (the training data will be resized to image_size*image_size)\n",
        "    latent_size = 32 # Latent resolution (ignore if not using latent diffusion)\n",
        "    diffusion_channels = 3 #1 for b&w images, 3 for RGB, 4 or more for latent diffusion\n",
        "\n",
        "    primal_batch_size = 128 #number of images in each mini-batch sampled for computing the primal loss\n",
        "    dual_batch_size = 64 #number of images in each mini-batch sampled for computing each constraint loss\n",
        "    eval_batch_size = 64 # how many samples to sample during evaluation step\n",
        "\n",
        "    num_epochs = 30 #number of total epochs\n",
        "    batches_per_epoch = 4 #number of mini-batches per epoch\n",
        "    primal_per_dual = 5 #number of primal descent steps after each update of the dual variables\n",
        "\n",
        "    save_model_epochs = 30 # number of epochs between each time the model is saved\n",
        "    save_plot_epochs = 20 # number of epochs between each time plots of relevant variables are saved\n",
        "    save_image_epochs = 10 # number of epochs between each time a batch of images generated by the diffusion model are saved\n",
        "    running_average_length = 5 #length of the running average for plotting average histograms of generated samples during training\n",
        "\n",
        "    num_gpus = 2 #number of gpus to split the training on using the 'accelerate' library\n",
        "\n",
        "    load_model_header = 'MODEL_NAME' # header of the initial model to load from the 'save_models_dir' directory. used if continuing training of a previusly trained model or fine-tuning a pre-trained model.\n",
        "    save_model_header = 'MODEL_NAME' # header to save the model with\n",
        "\n",
        "    gradient_accumulation_steps = 1\n",
        "\n",
        "    lr_primal = 1e-4 # the maximum primal learning rate\n",
        "    lr_dual_to_primal = 1000 #ratio of dual learning rate to primal learning rate\n",
        "    lr_warmup_steps = 500 # number of warmup steps to use in the learning rate scheduler\n",
        "\n",
        "    evaluate = True #set to True if you want the model to sample images from the diffusion model after every #save_image_epochs steps.\n",
        "    wandb_logging = False #set to True if you want to log relevant variables to wandb\n",
        "\n",
        "    architecture_size = 128 # the size of the denoising U-net model can be scaled up or down using this parameter\n",
        "\n",
        "\n",
        "    dataset_name = 'mnist' #name of the dataset to use for training. could be one of ['mnist', 'celeb-a', 'image-net']\n",
        "    include_default_color = True # whether to include the default background color (\"black\")\n",
        "\n",
        "    mixed_precision = \"fp16\"  # `no` for float32, `fp16` for automatic mixed precision\n",
        "\n",
        "    upload_to_github = False #If True, in addition to saving plots and imgs to file, will upload them to github repo\n",
        "    github_token = \" \"\n",
        "    github_repo = \" \"\n",
        "\n",
        "    output_dir = \"/content/output\"  # the local directory name to save everything\n",
        "    save_models_dir = \"models\" # the local directory to save trained models\n",
        "\n",
        "config = TrainingConfig()\n",
        "gdrive_path = \"/content/output/\"\n",
        "model_path = \"/content/trained_model_MODEL_NAME.pt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXXUEuO5JMO_"
      },
      "outputs": [],
      "source": [
        "#Classifier and VAE Setup\n",
        "\n",
        "#here we load a classifier corresponding to the dataset that is being used. For image-net we use a CLIP model as classifier and also load a pre-trained Variational AutoEncoder.\n",
        "\n",
        "if config.dataset_name == 'mnist':\n",
        "\n",
        "    path = 'farleyknight-org-username/vit-base-mnist'\n",
        "    classifier = ViTForImageClassification.from_pretrained(path)\n",
        "    classifier_processor = ViTImageProcessor.from_pretrained(path)\n",
        "    classifier.eval()\n",
        "\n",
        "elif config.dataset_name == 'celeb-a':\n",
        "\n",
        "    path = 'cledoux42/GenderNew_v002'\n",
        "    classifier = ViTForImageClassification.from_pretrained(path)\n",
        "    classifier_processor = ViTImageProcessor.from_pretrained(path)\n",
        "    classifier.eval()\n",
        "\n",
        "elif config.dataset_name == 'image-net':\n",
        "\n",
        "    #load CLIP model as classifier\n",
        "    clipmodel = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "    classifier_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "    tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "    classifier = clipmodel\n",
        "    classifier.eval()\n",
        "\n",
        "    #load VAE\n",
        "\n",
        "    vae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-mse\")\n",
        "    model_encoder = vae.encode\n",
        "    model_decoder = vae.decode\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPrZm_0eJMO_"
      },
      "outputs": [],
      "source": [
        "#Github Function\n",
        "def up_to_hub(image_path, content_path = 'images/uploaded_image.png', token = config.github_token, repo_name = config.github_repo):\n",
        "    #content_path is Path where you want the image to be uploaded in your repo\n",
        "\n",
        "    if config.upload_to_github == False:\n",
        "        return 1\n",
        "\n",
        "    # Initialize Github instance with your token\n",
        "    g = Github(token)\n",
        "\n",
        "    # Get the specific repo\n",
        "    repo = g.get_repo(repo_name)\n",
        "\n",
        "    # Read the image as binary\n",
        "    with open(image_path, \"rb\") as image_file:\n",
        "        data = image_file.read()\n",
        "        # Encode the binary data to base64 string\n",
        "        data_base64 = data\n",
        "\n",
        "    # Commit message\n",
        "    commit_message = 'Upload image via PyGithub'\n",
        "\n",
        "    # Upload the image\n",
        "    try:\n",
        "        # Check if the file already exists\n",
        "        contents = repo.get_contents(content_path)\n",
        "        repo.update_file(contents.path, commit_message, data_base64, contents.sha, branch=\"main\")\n",
        "        print(f\"Updated existing file: {content_path}\")\n",
        "    except:\n",
        "        # If the file does not exist, create it\n",
        "        repo.create_file(content_path, commit_message, data_base64, branch=\"main\")\n",
        "        print(f\"Created new file: {content_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHy-KtwjJMPA"
      },
      "outputs": [],
      "source": [
        "#Evaluate Function\n",
        "def evaluate(running_count, config, epoch, pipeline, mu, c, git_folder, accelerator, device, classifier_processor, classifier, classify = False, upload_images = False, final_epoch = False):\n",
        "\n",
        "    if config.dataset_name != 'image-net':\n",
        "\n",
        "        #sample images, in default PIL format\n",
        "        imgs = pipeline(\n",
        "        batch_size = config.eval_batch_size\n",
        "        ).images\n",
        "\n",
        "        if config.dataset_name == 'mnist':\n",
        "\n",
        "            imgs_ = [np.array(img) for img in imgs]\n",
        "            imgs_ = np.stack(imgs_, axis = 0)\n",
        "            imgs_ = torch.from_numpy(imgs_)\n",
        "            inputs = classifier_processor.preprocess(imgs_, do_rescale = True)\n",
        "\n",
        "\n",
        "        elif config.dataset_name == 'celeb-a':\n",
        "\n",
        "            imgs = pipeline(\n",
        "                batch_size = config.eval_batch_size, output_type = 'nd.array', num_inference_steps = 50\n",
        "                ).images\n",
        "\n",
        "            imgs = torch.from_numpy(imgs)\n",
        "            imgs = torch.permute(imgs, (0, 3, 1, 2))\n",
        "            imgs = imgs.to(device)\n",
        "\n",
        "            inputs = classifier_processor.preprocess(imgs, do_rescale = False)\n",
        "\n",
        "\n",
        "        outputs = classifier(torch.tensor(inputs['pixel_values']).to(device))\n",
        "        logits_per_image = outputs.logits\n",
        "        probs = logits_per_image.softmax(dim = 1)\n",
        "\n",
        "        predicted_nums = torch.argmax(probs, dim = 1)\n",
        "        arr = predicted_nums.cpu().numpy()\n",
        "\n",
        "        nclasses = np.shape(running_count)[0]\n",
        "\n",
        "        counts = np.bincount(arr, minlength = nclasses)\n",
        "\n",
        "        running_count += counts\n",
        "\n",
        "        if upload_images == True:\n",
        "\n",
        "            plt.bar(np.arange(0, nclasses, 1), counts, align='center')\n",
        "            plt.gca().set_xticks(np.arange(0, nclasses, 1))\n",
        "\n",
        "            plt.title('histogram of generated sample classes')\n",
        "            plt.savefig('class histogram.png')\n",
        "            plt.close()\n",
        "\n",
        "            up_to_hub('class histogram.png', content_path = git_folder_path + f\"/{epoch:04d}_histogram.png\")\n",
        "\n",
        "        if (int((epoch + 1) / config.save_image_epochs)%config.running_average_length == 0) and (upload_images == True):\n",
        "            plt.bar(np.arange(0, nclasses, 1), running_count, align='center', color = 'tab:red')\n",
        "            plt.gca().set_xticks(np.arange(0, nclasses, 1))\n",
        "\n",
        "            plt.title('running sum histogram of generated sample classes')\n",
        "            plt.savefig('running sum class histogram.png')\n",
        "            plt.close()\n",
        "\n",
        "            up_to_hub('running sum class histogram.png', content_path = git_folder_path + f\"/{epoch:04d}_rs_histogram.png\")\n",
        "\n",
        "        if final_epoch == True:\n",
        "            plt.bar(np.arange(0, nclasses, 1), running_count, align='center', color = 'olivedrab')\n",
        "            plt.gca().set_xticks(np.arange(0, nclasses, 1))\n",
        "\n",
        "            plt.title('final_epoch histogram of generated sample classes')\n",
        "            plt.savefig('running sum class histogram.png')\n",
        "            plt.close()\n",
        "\n",
        "            up_to_hub('running sum class histogram.png', content_path = git_folder_path + '/final_epoch_histogram.png')\n",
        "\n",
        "        rows = int(np.sqrt(config.eval_batch_size))\n",
        "        cols = int(np.sqrt(config.eval_batch_size))\n",
        "\n",
        "        if config.dataset_name == 'celeb-a':\n",
        "            imgs_decoded = [torchvision.transforms.functional.to_pil_image(imgs[k, :, :, :]) for k in range((imgs.shape)[0])]\n",
        "\n",
        "            # Make a grid out of the images\n",
        "            image_grid = make_image_grid(imgs_decoded, rows, cols)\n",
        "\n",
        "        else:\n",
        "            image_grid = make_image_grid(imgs, rows, cols)\n",
        "\n",
        "        # Save the images\n",
        "        test_dir = os.path.join(config.output_dir, \"samples\")\n",
        "        os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "        image_path = f\"{test_dir}/{epoch:04d}.png\"\n",
        "\n",
        "        image_grid.save(image_path)\n",
        "\n",
        "        up_to_hub(image_path, content_path = git_folder_path + f\"/{epoch:04d}.png\")\n",
        "\n",
        "    ###################################################\n",
        "    #######################LATENT######################\n",
        "    ###################################################\n",
        "\n",
        "    elif config.dataset_name == 'image-net':\n",
        "\n",
        "        device = accelerator.device\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            imgs = pipeline(\n",
        "            batch_size = config.eval_batch_size, output_type = 'nd.array', num_inference_steps = 50\n",
        "            ).images\n",
        "\n",
        "            imgs = torch.from_numpy(imgs)\n",
        "            imgs = torch.permute(imgs, (0, 3, 1, 2))\n",
        "            imgs = imgs.to(device)\n",
        "\n",
        "            vae.to(device)\n",
        "\n",
        "            model_decoder = vae.decode\n",
        "\n",
        "            print(imgs.shape)\n",
        "\n",
        "            a = 0.431\n",
        "            b = 36\n",
        "            imgs_ = (model_decoder((imgs - a)*b).sample).detach()\n",
        "\n",
        "            print(imgs_.shape)\n",
        "\n",
        "            #Classify\n",
        "\n",
        "            imgs_ = torch.clip((imgs_.permute(0, 2, 3, 1)), 0, 1)\n",
        "\n",
        "\n",
        "            labels = ['photo of a cassette player', 'photo of a tench fish', 'photo of a garbage truck', 'photo of a parachute', 'photo of a fench horn', 'photo of a english springer dog', 'photo of a golf ball', 'photo of a church', 'photo of a gas pump', 'photo of a chainsaw']\n",
        "            shorts = ['cass', 'fish', 'truck', 'parach', 'horn', 'dog', 'ball', 'church', 'pump', 'saw']\n",
        "\n",
        "            classifier_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\", device_map = device)\n",
        "            clipmodel = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\", device_map = device)\n",
        "\n",
        "            imgs_ = imgs_.to(device)\n",
        "\n",
        "            print(imgs_.device)\n",
        "\n",
        "            inputs = classifier_processor(text = labels, images = imgs_, do_rescale = False, do_convert_rgb = True, return_tensors = \"np\", padding = True)\n",
        "\n",
        "            inputs['pixel_values'] = torch.from_numpy(inputs['pixel_values']).float().to(device)\n",
        "            inputs['attention_mask'] = torch.from_numpy(inputs['attention_mask']).int().to(device)\n",
        "            inputs['input_ids'] = torch.from_numpy(inputs['input_ids']).int().to(device)\n",
        "\n",
        "            outputs = clipmodel(**inputs)\n",
        "            logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n",
        "            probs = logits_per_image.softmax(dim = 1)  # we can take the softmax to get the label probabilities\n",
        "            predicted_nums = torch.argmax(probs, dim = 1)\n",
        "            arr = predicted_nums.cpu().numpy()\n",
        "            counts = np.bincount(arr, minlength = 10)\n",
        "\n",
        "            running_count += counts\n",
        "\n",
        "            plt.bar(np.arange(0, 10), counts, align='center')\n",
        "            plt.gca().set_xticks(np.arange(0, 10), shorts)\n",
        "\n",
        "            plt.title('histogram of generated sample classes')\n",
        "            plt.savefig('class histogram.png')\n",
        "            plt.close()\n",
        "\n",
        "            up_to_hub('class histogram.png', content_path = git_folder_path + f\"/{epoch:04d}_histogram.png\")\n",
        "\n",
        "            rows = int(np.sqrt(config.eval_batch_size))\n",
        "            cols = int(np.sqrt(config.eval_batch_size))\n",
        "\n",
        "        ###########################################################\n",
        "        #Save and Upload the generated Latents\n",
        "        imgs_latents = [torchvision.transforms.functional.to_pil_image(imgs[k, :, :, :]) for k in range((imgs.shape)[0])]\n",
        "        image_grid = make_image_grid(imgs_latents, rows, cols)\n",
        "\n",
        "        test_dir = os.path.join(config.output_dir, \"samples\")\n",
        "        os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "        image_path = f\"{test_dir}/{epoch:04d}.png\"\n",
        "\n",
        "        image_grid.save(image_path)\n",
        "\n",
        "        up_to_hub(image_path, content_path = git_folder + f\"/{epoch:04d}_latents.png\")\n",
        "        ###########################################################\n",
        "        #Save and Upload the generated images (decoded latents)\n",
        "        imgs_ = imgs_.permute(0, 3, 1, 2)\n",
        "        imgs_decoded = [torchvision.transforms.functional.to_pil_image(imgs_[k, :, :, :]) for k in range((imgs_.shape)[0])]\n",
        "        image_grid = make_image_grid(imgs_decoded, rows, cols)\n",
        "        # Save the images\n",
        "        test_dir = os.path.join(config.output_dir, \"samples\")\n",
        "        os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "        image_path = f\"{test_dir}/{epoch:04d}.png\"\n",
        "        image_grid.save(image_path)\n",
        "\n",
        "        up_to_hub(image_path, content_path = git_folder + f\"/{epoch:04d}.png\")\n",
        "        ###########################################################\n",
        "        #Save and Upload the latent pixel values histogram\n",
        "\n",
        "\n",
        "        bat = (torch.flatten(imgs[0, :, :, :])).cpu().detach()\n",
        "        lat = bat.numpy()\n",
        "        plt.figure()\n",
        "        plt.hist(lat, bins = 50)\n",
        "\n",
        "        plt.savefig('lat.png')\n",
        "        image_path = 'lat.png'\n",
        "        up_to_hub(image_path, content_path = git_folder + f\"/{epoch:04d}_latents_histogram.png\")\n",
        "        plt.close()\n",
        "        ###########################################################\n",
        "\n",
        "    return running_count\n",
        "\n",
        "\n",
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, img_dir, transform = None, target_transform = None, use_images = False, images = None):\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        self.use_images = use_images\n",
        "        self.images = images\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.use_images:\n",
        "          return self.images.shape[0]\n",
        "        N = int(torch.load(self.img_dir + '/N').item())\n",
        "\n",
        "        return N\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.use_images:\n",
        "          image = self.images[idx]\n",
        "          label = 1\n",
        "        else:\n",
        "          img_path = os.path.join(self.img_dir, '/sample_' + str(idx) + '.pt')\n",
        "          img_tensor = torch.load(self.img_dir + '/sample_' + str(idx) + '.pt')\n",
        "          label =torch.load(self.img_dir + '/label_' + str(idx) + '.pt')\n",
        "\n",
        "          image = img_tensor\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(label)\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_gxLsYwJMPA"
      },
      "outputs": [],
      "source": [
        "#Fast Dataset Creation Function\n",
        "def create_datasets_fast(N_spc, config, dataloaders_only = False, save_jpgs_only = False, generated = False, model = None, noise_scheduler = None):\n",
        "\n",
        "    #################################\n",
        "    #Delete and recreate dataset directories\n",
        "    N_datasets = np.shape(N_spc)[0]\n",
        "    N_classes = np.shape(N_spc)[1]\n",
        "\n",
        "    dataset_names_list = []\n",
        "\n",
        "    if os.path.exists('temp_data'):\n",
        "        shutil.rmtree('temp_data')\n",
        "    os.mkdir('temp_data')\n",
        "\n",
        "    for i in range(N_datasets):\n",
        "        dataset_names_list += [\"dataset_\" + str(i)]\n",
        "\n",
        "    if save_jpgs_only == True:\n",
        "        dataset_names_list = [\"FID_baseline_dataset_\" + config.dataset_name]\n",
        "\n",
        "    if dataloaders_only == False:\n",
        "\n",
        "        for new_folder_name in dataset_names_list:\n",
        "            # Specify the name of the new folder\n",
        "            if os.path.exists(new_folder_name):\n",
        "                shutil.rmtree(new_folder_name)\n",
        "            # Create a new directory in the current working directory\n",
        "            os.mkdir(new_folder_name)\n",
        "\n",
        "        #################################\n",
        "        #create datasets\n",
        "        if config.dataset_name == 'image-net':\n",
        "            dataset_orig = load_dataset(\"frgfm/imagenette\", '320px', split=\"train\")\n",
        "        elif config.dataset_name == 'mnist':\n",
        "            dataset_orig = load_dataset(\"ylecun/mnist\", split=\"train\")\n",
        "        elif config.dataset_name == 'celeb-a':\n",
        "            dataset_orig = load_dataset(\"tpremoli/CelebA-attrs\", split=\"validation\")\n",
        "            dataset_orig = dataset_orig.rename_column('Male', 'label')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        for i in range(N_datasets):\n",
        "\n",
        "            path = dataset_names_list[i]\n",
        "            n = 0\n",
        "\n",
        "            for j in range(N_classes):\n",
        "\n",
        "                if N_spc[i, j] == 0:\n",
        "                    continue\n",
        "\n",
        "\n",
        "                if config.dataset_name == 'celeb-a':\n",
        "                    ds_filtered = dataset_orig.filter(lambda example: example[\"label\"] == int((j-0.5)*2))\n",
        "                else:\n",
        "                    ds_filtered = dataset_orig.filter(lambda example: example[\"label\"] == j)\n",
        "                ds_filtered = ds_filtered.filter(lambda example, idx: idx < N_spc[i, j], with_indices=True)\n",
        "                ds_filtered = ds_filtered.with_format(\"torch\")\n",
        "\n",
        "\n",
        "                dataloader = DataLoader(ds_filtered, batch_size=1)\n",
        "\n",
        "                trans = torchvision.transforms.Resize((config.image_size, config.image_size))\n",
        "\n",
        "\n",
        "                for batch in dataloader:\n",
        "\n",
        "                    img = trans(batch['image'][0, :, :, :])\n",
        "\n",
        "                    if img.shape[0] == 1:\n",
        "                        img = img.repeat(3, 1, 1)\n",
        "\n",
        "                    if config.dataset_name == 'image-net':\n",
        "                        torch.save(img, 'temp_data/' + 'sample_' + str(n) + '.pt')\n",
        "                    if config.dataset_name != 'image-net' and save_jpgs_only == False:\n",
        "                        img = img/255\n",
        "                        torch.save(img, path + '/sample_' + str(n) + '.pt')\n",
        "                        torch.save(j, path + '/label_' + str(n) + '.pt')\n",
        "                        img = img*255\n",
        "\n",
        "                    if save_jpgs_only == True:\n",
        "                        img = img/255\n",
        "                        img = img.double()\n",
        "                        torchvision.utils.save_image(img, path + '/sample_' + str(n) + '.jpg')\n",
        "\n",
        "                    n += 1\n",
        "\n",
        "                N = torch.tensor(n)\n",
        "                torch.save(N, 'temp_data/N')\n",
        "                torch.save(N, path + '/N')\n",
        "\n",
        "                if save_jpgs_only == True:\n",
        "                    continue\n",
        "\n",
        "                if config.dataset_name == 'image-net':\n",
        "\n",
        "                    ds = CustomImageDataset(img_dir = 'temp_data')\n",
        "                    dataloader = torch.utils.data.DataLoader(ds, batch_size = 16, shuffle = True)\n",
        "\n",
        "                    device ='cuda:0'\n",
        "                    vae.to(device)\n",
        "                    model_encoder = vae.encode\n",
        "\n",
        "                    n = 0\n",
        "\n",
        "                    with torch.no_grad():\n",
        "                        for batch in dataloader:\n",
        "                            batch[0] = (batch[0]/255).to(device).detach()\n",
        "                            lat = model_encoder(batch[0])\n",
        "                            lat = lat.to_tuple()[0]\n",
        "                            lat = (lat.mean).detach()\n",
        "                            for k in range(lat.shape[0]):\n",
        "                                torch.save(lat[k, :, :, :].detach(), path + '/sample_' + str(n) + '.pt')\n",
        "                                n += 1\n",
        "\n",
        "\n",
        "            N = torch.tensor(n)\n",
        "\n",
        "            torch.save(N, path + '/N')\n",
        "    #Now we create the dataloaders\n",
        "    dataset_list = []\n",
        "    preprocess = transforms.Compose(\n",
        "    [\n",
        "        transforms.Normalize([0.5], [0.5]),\n",
        "    ]\n",
        "    )\n",
        "\n",
        "    for i in range(N_datasets):\n",
        "        if config.dataset_name != 'image-net':\n",
        "            dataset_list += [CustomImageDataset(img_dir = dataset_names_list[i], transform = preprocess)]\n",
        "        else:\n",
        "            dataset_list += [CustomImageDataset(img_dir = dataset_names_list[i])]\n",
        "\n",
        "\n",
        "\n",
        "    ######################################\n",
        "    train_dataloader_list = []\n",
        "\n",
        "    for i in range(len(dataset_list)):\n",
        "        train_dataloader_list += [torch.utils.data.DataLoader(dataset_list[i], batch_size = config.primal_batch_size, shuffle = True)]\n",
        "\n",
        "\n",
        "    return train_dataloader_list\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### MNIST CLASSIFIER\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        # First convolutional layer\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        # Second convolutional layer\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        # Shortcut connection to match dimensions\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = F.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        out += self.shortcut(residual)\n",
        "        out = F.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class MNISTResNet(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(MNISTResNet, self).__init__()\n",
        "\n",
        "        # Initial input processing\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "\n",
        "        # Residual blocks with increasing channels and decreasing spatial dimensions\n",
        "        self.layer1 = self._make_layer(16, 32, stride=1)\n",
        "        self.layer2 = self._make_layer(32, 64, stride=1)\n",
        "        self.layer3 = self._make_layer(64, 64, stride=1)\n",
        "\n",
        "        # Additional downsampling to reduce feature map size further\n",
        "        self.pool1 = nn.MaxPool2d((2, 2))\n",
        "        self.pool2 = nn.MaxPool2d((2, 2))\n",
        "        self.pool3 = nn.MaxPool2d((2, 2))\n",
        "\n",
        "        # Final classifier - 64 * 4 * 4 = 1024\n",
        "        self.fc1 = nn.Linear(64 * 4 * 4, 128)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def _make_layer(self, in_channels, out_channels, stride):\n",
        "        layers = []\n",
        "        # First block may downsample\n",
        "        layers.append(ResidualBlock(in_channels, out_channels, stride))\n",
        "        # Second block maintains dimensions\n",
        "        layers.append(ResidualBlock(out_channels, out_channels, 1))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initial processing\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "\n",
        "        # Residual blocks\n",
        "        out = self.layer1(out)\n",
        "        out = self.pool1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.pool2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.pool3(out)\n",
        "\n",
        "        # Flatten and classify\n",
        "        out = torch.flatten(out, 1)\n",
        "        out = self.fc1(out)\n",
        "        out = F.relu(out)\n",
        "        out = self.fc2(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "OtYH3iC-feTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MNIST CLASSIFIER TRAINING\n",
        "def train_mnist_classifer(config, model, optimizer, lr_scheduler, train_dataloader, epochs = 100):\n",
        "\n",
        "    # Initialize accelerator and tensorboard logging\n",
        "    accelerator = Accelerator(\n",
        "        mixed_precision=config.mixed_precision,\n",
        "        gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
        "        log_with=\"tensorboard\",\n",
        "        project_dir=os.path.join(\"FOLDER_NAME\", \"logs\"),\n",
        "    )\n",
        "\n",
        "    device = accelerator.device\n",
        "\n",
        "    model, optimizer, dataloader, lr_scheduler = accelerator.prepare(\n",
        "        model, optimizer, train_dataloader, lr_scheduler\n",
        "    )\n",
        "\n",
        "    iterator = cycle(dataloader)\n",
        "\n",
        "    global_step = 0\n",
        "\n",
        "    #Actually start training\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        progress_bar = tqdm(total = config.batches_per_epoch, disable = not accelerator.is_local_main_process, position = 0)\n",
        "        progress_bar.set_description(f\"Epoch {epoch}\")\n",
        "\n",
        "        for step in range(config.batches_per_epoch):\n",
        "\n",
        "            device = accelerator.device\n",
        "\n",
        "            train_features, labels = next(iterator)\n",
        "            Clean_images = train_features.to(device)\n",
        "            Noises = torch.randn(Clean_images.shape, device = Clean_images.device)\n",
        "            Batch_sizes = Clean_images.shape[0]\n",
        "\n",
        "            with accelerator.accumulate(model):\n",
        "                digit_preds = model(Clean_images)\n",
        "                Losses = F.cross_entropy(digit_preds, labels, reduction = 'none')\n",
        "\n",
        "                loss_scales = 1.\n",
        "\n",
        "                loss = (loss_scales * Losses).mean()\n",
        "\n",
        "                accelerator.backward(loss)\n",
        "\n",
        "                accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                optimizer.step()\n",
        "                lr_scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "\n",
        "            progress_bar.update(1)\n",
        "            logs = {\"loss\": loss.detach().item(), \"step\": global_step}\n",
        "            progress_bar.set_postfix(**logs)\n",
        "            accelerator.log(logs, step=global_step)\n",
        "            global_step += 1"
      ],
      "metadata": {
        "id": "7pONle9upCon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def evaluate_classifier(dataloader, classifier):\n",
        "  iterator = cycle(dataloader)\n",
        "  device = \"cuda:0\"\n",
        "  train_features, labels = next(iterator)\n",
        "  true_labels = labels.to(device)\n",
        "  Clean_images = train_features.to(device)\n",
        "  digit_preds = classifier(Clean_images.half())\n",
        "  preds = torch.softmax(digit_preds, dim = 1)\n",
        "  return preds\n"
      ],
      "metadata": {
        "id": "pUmzRYaXT57-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#### TRAIN MNIST CLASSIFIER ####\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "classifier_dataset_spc = np.zeros((1, 10)) # a tensor specifying the number of Samples Per Class (spc) for each dataset\n",
        "classifier_dataset_spc[0, :] = 256\n",
        "classifer_dataloaders = create_datasets_fast(classifier_dataset_spc, config, False, False)\n",
        "\n",
        "mnist_classifier = MNISTResNet(num_classes = 10)\n",
        "optimizer = torch.optim.AdamW(mnist_classifier.parameters(), lr = 0.003)\n",
        "lr_scheduler = LambdaLR(optimizer, lr_lambda=lambda step: 1.0)\n",
        "\n",
        "train_mnist_classifer(config, mnist_classifier, optimizer, lr_scheduler, classifer_dataloaders[0], epochs = 100)"
      ],
      "metadata": {
        "id": "lKphjosVlPCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for param in mnist_classifier.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "FwM_leC3h1Ie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate classifier\n",
        "result = evaluate_classifier(classifer_dataloaders[0], mnist_classifier)"
      ],
      "metadata": {
        "id": "Qw0lh4JgX_XK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "id": "I7kU5Gcfap70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPEup3ZlJMPA"
      },
      "outputs": [],
      "source": [
        "#Model Setup\n",
        "\n",
        "#A 2D UNet model that takes a noisy sample and a timestep and returns a sample shaped output.\n",
        "model = UNet2DModel(\n",
        "    sample_size = config.latent_size,  # the target image resolution\n",
        "    in_channels = config.diffusion_channels,  # the number of input channels, 3 for RGB images\n",
        "    out_channels = config.diffusion_channels,  # the number of output channels, 3 for RGB images\n",
        "    layers_per_block = 2,  # how many ResNet layers to use per UNet block\n",
        "    block_out_channels = (config.architecture_size, config.architecture_size, 2*config.architecture_size, 2*config.architecture_size, 4*config.architecture_size, 4*config.architecture_size),  # the number of output channels for each UNet block\n",
        "    down_block_types=(\n",
        "        \"DownBlock2D\",  # a regular ResNet downsampling block\n",
        "        \"DownBlock2D\",\n",
        "        \"DownBlock2D\",\n",
        "        \"DownBlock2D\",\n",
        "        \"AttnDownBlock2D\",  # a ResNet downsampling block with spatial self-attention\n",
        "        \"DownBlock2D\",\n",
        "    ),\n",
        "    up_block_types=(\n",
        "        \"UpBlock2D\",  # a regular ResNet upsampling block\n",
        "        \"AttnUpBlock2D\",  # a ResNet upsampling block with spatial self-attention\n",
        "        \"UpBlock2D\",\n",
        "        \"UpBlock2D\",\n",
        "        \"UpBlock2D\",\n",
        "        \"UpBlock2D\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "frozen_model = UNet2DModel(\n",
        "    sample_size = config.latent_size,  # the target image resolution\n",
        "    in_channels = config.diffusion_channels,  # the number of input channels, 3 for RGB images\n",
        "    out_channels = config.diffusion_channels,  # the number of output channels, 3 for RGB images\n",
        "    layers_per_block = 2,  # how many ResNet layers to use per UNet block\n",
        "    block_out_channels = (config.architecture_size, config.architecture_size, 2*config.architecture_size, 2*config.architecture_size, 4*config.architecture_size, 4*config.architecture_size),  # the number of output channels for each UNet block\n",
        "    down_block_types=(\n",
        "        \"DownBlock2D\",  # a regular ResNet downsampling block\n",
        "        \"DownBlock2D\",\n",
        "        \"DownBlock2D\",\n",
        "        \"DownBlock2D\",\n",
        "        \"AttnDownBlock2D\",  # a ResNet downsampling block with spatial self-attention\n",
        "        \"DownBlock2D\",\n",
        "    ),\n",
        "    up_block_types=(\n",
        "        \"UpBlock2D\",  # a regular ResNet upsampling block\n",
        "        \"AttnUpBlock2D\",  # a ResNet upsampling block with spatial self-attention\n",
        "        \"UpBlock2D\",\n",
        "        \"UpBlock2D\",\n",
        "        \"UpBlock2D\",\n",
        "        \"UpBlock2D\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "#loading parameters of pre-trained model if needed\n",
        "if config.load_model_header != 'None':\n",
        "    device = torch.device('cpu')\n",
        "    state_dict = torch.load(model_path, map_location = device)\n",
        "\n",
        "    # load params\n",
        "    model.load_state_dict(state_dict)\n",
        "\n",
        "    # Frozen model\n",
        "    frozen_model.load_state_dict(state_dict)\n",
        "    for param in frozen_model.parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "    print('loaded model ' + config.load_model_header)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Dataset with Sampled Images\n",
        "def sample_images_with_model(model, noise_scheduler, total_samples, accelerator = None, as_torch=True, batch_size = 128):\n",
        "  if not accelerator:\n",
        "    accelerator = Accelerator(\n",
        "          mixed_precision=config.mixed_precision,\n",
        "          gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
        "          log_with=\"tensorboard\",\n",
        "          project_dir=os.path.join(\"FOLDER_NAME\", \"logs\"),\n",
        "      )\n",
        "    model = accelerator.prepare(model)\n",
        "  pipeline = DDIMPipeline(unet = accelerator.unwrap_model(model), scheduler = noise_scheduler)\n",
        "  imgs_ = []\n",
        "  for i in range(0, total_samples, batch_size):\n",
        "    imgs_.extend(pipeline(batch_size = min(batch_size, total_samples - i)).images)\n",
        "  if as_torch:\n",
        "    imgs_ = [np.array(img) for img in imgs_]\n",
        "    imgs_ = np.stack(imgs_, axis = 0)\n",
        "    imgs_ = torch.from_numpy(imgs_)\n",
        "  return imgs_\n",
        "\n",
        "def create_dataset_sampled(model, noise_scheduler, total_samples, accelerator = None):\n",
        "  samples = sample_images_with_model(model, noise_scheduler, total_samples, accelerator)\n",
        "  preprocess = transforms.Compose(\n",
        "    [\n",
        "        transforms.Normalize([0.5], [0.5]),\n",
        "    ]\n",
        "  )\n",
        "  dataloader = torch.utils.data.DataLoader(\n",
        "      CustomImageDataset(\n",
        "          img_dir = \"\",\n",
        "          images = samples.permute((0, 3, 1, 2))/255,\n",
        "          transform=preprocess,\n",
        "          use_images = True),\n",
        "      batch_size = config.primal_batch_size,\n",
        "      shuffle = True)\n",
        "  return dataloader"
      ],
      "metadata": {
        "id": "unzNlUeDr-Lr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4qDGTOLJMPB"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Training loop function\n",
        "def finetune_loop(config, git_folder_path, model, classifier, mnist_classifier, noise_scheduler, optimizer, train_dataloaders, lr_scheduler, mu_init, b_init,\n",
        "                  discriminative_class = 4,\n",
        "                  delta = 0.1,\n",
        "                  initial_penalty_lambda = 0.05, # Start small\n",
        "                  lambda_increase_every = 10,   # Increase every n iterations\n",
        "                  num_optimization_per_batch = 101,\n",
        "                  lambda_growth_factor = 1.3,\n",
        "                  early_stop_threshold = 0.004\n",
        "                  ):\n",
        "\n",
        "    # Initialize accelerator and tensorboard logging\n",
        "    accelerator = Accelerator(\n",
        "        mixed_precision=config.mixed_precision,\n",
        "        gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
        "        log_with=\"tensorboard\",\n",
        "        project_dir=os.path.join(\"FOLDER_NAME\", \"logs\"),\n",
        "    )\n",
        "\n",
        "    device = accelerator.device\n",
        "\n",
        "    model, optimizer, lr_scheduler = accelerator.prepare(\n",
        "        model, optimizer, lr_scheduler\n",
        "    )\n",
        "\n",
        "    # Prepare dataloader and iterator\n",
        "    dataloaders = accelerator.prepare(train_dataloaders)\n",
        "    iterators = cycle(dataloaders)\n",
        "\n",
        "    global_step = 0\n",
        "\n",
        "    running_count = np.zeros(10)\n",
        "\n",
        "    ddim_scheduler = DDIMScheduler.from_config(noise_scheduler.config)\n",
        "    ddim_scheduler.set_timesteps(50)\n",
        "\n",
        "    loss_hist = torch.zeros(int(config.num_epochs))\n",
        "\n",
        "    classifier = classifier.to(device)\n",
        "\n",
        "    ##important\n",
        "    pipeline = DDIMPipeline(unet = accelerator.unwrap_model(model), scheduler = noise_scheduler)\n",
        "\n",
        "    evaluate(running_count, config, 0, pipeline, 0.0, 0.0, git_folder_path, accelerator, device, classifier_processor, classifier, classify = True)\n",
        "\n",
        "    #Actually start training\n",
        "    for epoch in range(config.num_epochs):\n",
        "\n",
        "        progress_bar = tqdm(total = config.batches_per_epoch, disable = not accelerator.is_local_main_process, position = 0)\n",
        "        progress_bar.set_description(f\"Epoch {epoch}\")\n",
        "\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for step in range(config.batches_per_epoch):\n",
        "\n",
        "            device = accelerator.device\n",
        "\n",
        "            train_features, _ = next(iterators)\n",
        "            Clean_images = train_features.to(device)\n",
        "            Noises = torch.randn(Clean_images.shape, device = Clean_images.device)\n",
        "            Batch_sizes = Clean_images.shape[0]\n",
        "            Timesteps = torch.randint(\n",
        "                        0, noise_scheduler.config.num_train_timesteps, (Batch_sizes,), device = Clean_images.device,\n",
        "                        dtype = torch.int64\n",
        "                    )\n",
        "            Noisy_images = noise_scheduler.add_noise(Clean_images, Noises, Timesteps)\n",
        "            Train_labels = torch.from_numpy(np.array([discriminative_class] * Batch_sizes)).to(device)\n",
        "\n",
        "            penalty_lambda = initial_penalty_lambda\n",
        "\n",
        "            batch_total_loss = 0.0\n",
        "\n",
        "            for n in range(num_optimization_per_batch):\n",
        "\n",
        "              with accelerator.accumulate(model):\n",
        "\n",
        "                  #The Lagrangian\n",
        "\n",
        "                  Noise_preds = model(Noisy_images, Timesteps, return_dict = False)[0]\n",
        "\n",
        "                  timestep = Timesteps.cpu()\n",
        "\n",
        "                  # compute alphas, betas\n",
        "                  alpha_prod_t = ddim_scheduler.alphas_cumprod[timestep][:, None, None, None].to(device)\n",
        "                  beta_prod_t = 1. - alpha_prod_t\n",
        "                  sample = Noisy_images\n",
        "\n",
        "                  pred_original_sample = (sample - beta_prod_t ** (0.5) * Noise_preds) / alpha_prod_t ** (0.5)\n",
        "                  pred_original_sample = pred_original_sample.clamp(\n",
        "                      -ddim_scheduler.config.clip_sample_range, ddim_scheduler.config.clip_sample_range\n",
        "                  )\n",
        "\n",
        "                  # Calculate \\grad log p(y | x)\n",
        "                  pred_probs = F.softmax(mnist_classifier(pred_original_sample), dim = 1) # p(y | x)\n",
        "\n",
        "                  one_hot_vec = F.one_hot(Train_labels, num_classes = 10)\n",
        "\n",
        "                  # PENALTY TERM: p(y | x) for the specific labels\n",
        "                  pred_prev_classification = (pred_probs * one_hot_vec).sum(dim = 1)\n",
        "                  penalty = torch.mean((Timesteps < 800) * torch.relu(pred_prev_classification - delta)**2) ** 2\n",
        "\n",
        "                  early_stop = penalty <= early_stop_threshold\n",
        "\n",
        "                  diffusion_loss = F.mse_loss(Noise_preds, Noises)\n",
        "\n",
        "                  loss = diffusion_loss + penalty_lambda * torch.norm(diffusion_loss) / (torch.norm(penalty) + 1e-3) * penalty\n",
        "\n",
        "                  batch_total_loss += loss.detach()\n",
        "\n",
        "                  accelerator.backward(loss)\n",
        "\n",
        "                  accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                  optimizer.step()\n",
        "                  lr_scheduler.step()\n",
        "                  optimizer.zero_grad()\n",
        "\n",
        "                  if early_stop:\n",
        "                    break\n",
        "\n",
        "              # Gradually increase lambda\n",
        "              if n % lambda_increase_every == 0:\n",
        "                  penalty_lambda *= lambda_growth_factor\n",
        "\n",
        "            if not early_stop:\n",
        "              print(\"Batch did not converge with penalty:\")\n",
        "              print(penalty)\n",
        "\n",
        "            batch_average_loss = batch_total_loss / num_optimization_per_batch\n",
        "            total_loss += batch_average_loss\n",
        "            progress_bar.update(1)\n",
        "            logs = {\"loss\": batch_average_loss }\n",
        "            progress_bar.set_postfix(**logs)\n",
        "            accelerator.log(logs, step=global_step)\n",
        "            global_step += 1\n",
        "\n",
        "        loss_hist[epoch] = total_loss / config.batches_per_epoch\n",
        "\n",
        "        # After each epoch you optionally sample some demo images with evaluate() and save the model\n",
        "\n",
        "        if accelerator.is_main_process and config.evaluate == True:\n",
        "\n",
        "            classifier = classifier.to(device)\n",
        "\n",
        "            ##important\n",
        "            pipeline = DDIMPipeline(unet = accelerator.unwrap_model(model), scheduler = noise_scheduler)\n",
        "\n",
        "\n",
        "            if ((epoch + 1) % config.save_image_epochs == 0) and ((epoch + 1) % config.save_plot_epochs != 0):\n",
        "                #just draw samples and show them in a grid\n",
        "                running_count = evaluate(running_count, config, epoch, pipeline, 0.0, 0.0, git_folder_path, accelerator, device, classifier_processor, classifier, classify = True)\n",
        "\n",
        "            if epoch == config.num_epochs - 1:\n",
        "                #In the last epoch sample lots of images and check their classes\n",
        "                running_count_final_epoch = np.zeros(10)\n",
        "\n",
        "                for k in range(20):\n",
        "                    running_count_final_epoch = evaluate(running_count_final_epoch, config, epoch, pipeline, 0.0, 0.0, git_folder_path, accelerator, device, classifier_processor, classifier, classify = True, upload_images = False)\n",
        "\n",
        "                running_count_final_epoch = evaluate(running_count_final_epoch, config, epoch, pipeline, 0.0, 0.0, git_folder_path, accelerator, device, classifier_processor, classifier, classify = True, upload_images = False, final_epoch = True)\n",
        "\n",
        "            if ((epoch + 1) % config.save_plot_epochs == 0 ) or (epoch == config.num_epochs - 1):\n",
        "                #draw samples and show them in a grid. Also update plots and add a class histogram and save model\n",
        "                running_count = evaluate(running_count, config, epoch, pipeline, mu_init, b_init, git_folder_path, accelerator, device, classifier_processor, classifier, classify = True)\n",
        "\n",
        "                #Save model\n",
        "                torch.save(model.state_dict(), gdrive_path + 'models/trained_model_' + config.save_model_header + '.pt')\n",
        "\n",
        "                #######################plots of histories of important stuff#######################\n",
        "                with torch.no_grad():\n",
        "\n",
        "                    plt.plot(loss_hist)\n",
        "                    plt.ylabel('loss')\n",
        "                    plt.xlabel('# epochs')\n",
        "                    plt.title('loss history')\n",
        "                    plt.savefig(gdrive_path + 'loss_hist.png')\n",
        "                    plt.close()\n",
        "\n",
        "                    torch.save(loss_hist, gdrive_path + 'loss_hist.pt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17Uwz_CWJMPB"
      },
      "outputs": [],
      "source": [
        "# Run\n",
        "\n",
        "noise_scheduler = DDPMScheduler(num_train_timesteps = 1000)\n",
        "total_samples = 512\n",
        "discriminative_class = 4\n",
        "Train_dataloaders = [create_dataset_sampled(frozen_model, noise_scheduler, total_samples)]\n",
        "################HYPERPARAMS##################\n",
        "eta_resilience = 0.1\n",
        "\n",
        "config.num_epochs = 100\n",
        "config.batches_per_epoch = int((total_samples/config.primal_batch_size))\n",
        "if config.batches_per_epoch == 0:\n",
        "    config.batches_per_epoch = 1\n",
        "# config.batches_per_epoch = 1\n",
        "config.primal_per_dual = 2\n",
        "config.save_image_epochs = 10\n",
        "config.save_plot_epochs = 500\n",
        "config.running_average_length = 5\n",
        "config.lr_primal = 0.0003\n",
        "config.lr_dual_to_primal = 1000\n",
        "\n",
        "delta = 0.1\n",
        "initial_penalty_lambda = 0.05 # Start small\n",
        "lambda_increase_every = 10   # Increase every n iterations\n",
        "num_optimization_per_batch = 151\n",
        "lambda_growth_factor = 1.2\n",
        "early_stop_threshold = 0.005\n",
        "\n",
        "# mu_init_scalar = 0\n",
        "mu_init_scalar = 0.0\n",
        "b_init_scalar = 0.0\n",
        "\n",
        "if config.adaptation == True:\n",
        "    b_init = b_init_scalar*torch.ones(2)\n",
        "    mu_init = torch.tensor([1, 0])\n",
        "else:\n",
        "    mu_init = mu_init_scalar*torch.zeros(1)\n",
        "    mu_init[0] = 1\n",
        "    b_init = b_init_scalar*torch.ones(1)\n",
        "\n",
        "constrained = False\n",
        "resilient = False\n",
        "\n",
        "alpha = 0.085\n",
        "\n",
        "#OPTIMIZER + LR_SCHEDULER\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr = config.lr_primal)\n",
        "lr_scheduler = get_cosine_schedule_with_warmup(\n",
        "    optimizer = optimizer,\n",
        "    num_warmup_steps = config.lr_warmup_steps,\n",
        "    num_training_steps = (config.batches_per_epoch * config.num_epochs),\n",
        "    num_cycles = 0.25\n",
        ")\n",
        "\n",
        "\n",
        "############################################\n",
        "\n",
        "git_folder_path = 'FOLDER_NAME'\n",
        "config.save_model_header = 'MODEL_NAME'\n",
        "\n",
        "if config.wandb_logging == True:\n",
        "    wandb.init(\n",
        "        project = 'PROJECT_NAME',\n",
        "        config = config\n",
        "    )\n",
        "\n",
        "# Open the file in write mode and save the string\n",
        "with open(gdrive_path + 'run_info.txt', 'w') as file:\n",
        "    file.write('num_epochs = ' + str(config.num_epochs) + \"\\n\")\n",
        "    file.write('batches_per_epoch = ' + str(config.batches_per_epoch) + \"\\n\")\n",
        "    file.write('primal_batch_size = ' + str(config.primal_batch_size) + \"\\n\")\n",
        "    file.write('dataset_size = ' + str(total_samples)  + \"\\n\")\n",
        "    file.write('primal_per_dual = ' + str(config.primal_per_dual) + \"\\n\")\n",
        "    file.write('save_image_epochs = ' + str(config.save_image_epochs) + \"\\n\")\n",
        "    file.write('save_plot_epochs = ' + str(config.save_plot_epochs) + \"\\n\")\n",
        "    file.write('running_average_length_for_classification_of_generated_samples = ' + str(config.running_average_length) + \"\\n\")\n",
        "    file.write('lr_dual_to_primal = ' + str(config.lr_dual_to_primal) + \"\\n\")\n",
        "    file.write('eta_resilience = ' + str(eta_resilience) + \"\\n\")\n",
        "    file.write('eta_main = ' + str(config.lr_primal)  + \"\\n\")\n",
        "    file.write('alpha (const relaxation cost) = ' + str(alpha) + \"\\n\")\n",
        "    file.write('initial mu = ' + str(mu_init) + \"\\n\")\n",
        "    file.write('initial b = ' + str(b_init) + \"\\n\")\n",
        "    file.write('save_model_header = ' + str(config.save_model_header)  + \"\\n\")\n",
        "    file.write('load_model_header = ' + str(config.load_model_header)  + \"\\n\")\n",
        "    file.write('constrained = ' + str(constrained)  + \"\\n\")\n",
        "    file.write('resilient = ' + str(resilient)  + \"\\n\")\n",
        "    file.write(f'delta = {delta}\\n')\n",
        "    file.write(f'initial_penalty_lambda = {initial_penalty_lambda}\\n')\n",
        "    file.write(f'lambda_increase_every = {lambda_increase_every}\\n')\n",
        "    file.write(f'num_optimization_per_batch = {num_optimization_per_batch}\\n')\n",
        "    file.write(f'lambda_growth_factor = {lambda_growth_factor}\\n')\n",
        "    file.write(f'early_stop_threshold = {early_stop_threshold}\\n')\n",
        "\n",
        "\n",
        "# up_to_hub('run_info.txt', content_path = git_folder_path + '/run_info.txt')\n",
        "\n",
        "# def train_loop(config, git_folder_path, model, classifier, noise_scheduler, optimizer, train_dataloaders, lr_scheduler, mu_init, b_init, constrained = False, resilient = False, alpha = 0,  model_pretrained = None):\n",
        "args = (config, git_folder_path, model, classifier, mnist_classifier,\n",
        "        noise_scheduler, optimizer, Train_dataloaders[0], lr_scheduler, mu_init, b_init,\n",
        "        discriminative_class, delta, initial_penalty_lambda, lambda_increase_every,\n",
        "        num_optimization_per_batch, lambda_growth_factor, early_stop_threshold)\n",
        "\n",
        "# result = finetune_loop(*args)\n",
        "\n",
        "\n",
        "notebook_launcher(finetune_loop, args, num_processes = config.num_gpus)\n",
        "\n",
        "#Ignore if not having issues with server ports\n",
        "# portnum = 8000\n",
        "# notebook_launcher(train_loop, args, num_processes = config.num_gpus, use_port= str(portnum))\n",
        "\n",
        "if config.wandb_logging == True:\n",
        "    wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please see `output/samples` for generated samples."
      ],
      "metadata": {
        "id": "wXEV7YJVRHIh"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iT7F358CRUhq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}